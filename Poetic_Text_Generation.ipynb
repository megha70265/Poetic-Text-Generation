{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPuQ01wHUdDVWFcQWt7yx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megha70265/Poetic-Text-Generation/blob/main/Poetic_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP56es4qnjYq"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYM5aOJHorde",
        "outputId": "4301e128-ca3f-4070-e59e-07f60c14162c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=text.lower()\n",
        "text = text[300000:800000]"
      ],
      "metadata": {
        "id": "Q1Kw7ERYo7G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(set(text))\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))"
      ],
      "metadata": {
        "id": "aaoOCQMkpSsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sorted set of characters is created. In python set no value appears more than once. We create two dictionaries one for accessing index of a character by charcter and second character by index. This way a representation for each charcter can be created."
      ],
      "metadata": {
        "id": "xDtF0LsSpsIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3\n",
        "\n",
        "sentences = []\n",
        "next_char = []"
      ],
      "metadata": {
        "id": "K4PHkm7DpkpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define how long a sequence shall be and also how many characters we will step further to start the next sentence. What we try to do here is to take sentences and then save the next letter as the training data."
      ],
      "metadata": {
        "id": "qbHVdOVRqz6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "    sentences.append(text[i: i + SEQ_LENGTH])\n",
        "    next_char.append(text[i + SEQ_LENGTH])"
      ],
      "metadata": {
        "id": "SVFQDcaJqCtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((len(sentences), SEQ_LENGTH,\n",
        "              len(characters)), dtype=bool)\n",
        "y = np.zeros((len(sentences),\n",
        "              len(characters)), dtype=bool)\n",
        "\n",
        "for i, satz in enumerate(sentences):\n",
        "    for t, char in enumerate(satz):\n",
        "        x[i, t, char_to_index[char]] = 1\n",
        "    y[i, char_to_index[next_char[i]]] = 1"
      ],
      "metadata": {
        "id": "7j_ywL2_quTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Recurent Network"
      ],
      "metadata": {
        "id": "VJYD85-yrfOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import Activation, Dense, LSTM"
      ],
      "metadata": {
        "id": "0WuOncX6rpFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(LSTM(128,input_shape=(SEQ_LENGTH,len(characters))))\n",
        "\n",
        "model.add(Dense(len(characters)))\n",
        "\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "ZCkLdo1yrxkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.01))\n",
        "\n",
        "model.fit(x, y, batch_size=256, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtY3ijlYsSXC",
        "outputId": "674e0979-4026-4ed9-c969-89dee5e937df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1302/1302 [==============================] - 235s 179ms/step - loss: 2.5253\n",
            "Epoch 2/4\n",
            "1302/1302 [==============================] - 227s 174ms/step - loss: 2.1678\n",
            "Epoch 3/4\n",
            "1302/1302 [==============================] - 244s 188ms/step - loss: 2.0457\n",
            "Epoch 4/4\n",
            "1302/1302 [==============================] - 233s 179ms/step - loss: 1.9493\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78af3c29a230>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "VfkCAsZ1uWBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It basically just picks one of the characters from the output. As parameters it takes the result of the prediction and a temperature. This temperature indicates how risky the pick shall be. If we have a high temperature, we will pick one of the less likely characters. A low temperature will cause a conservative choice."
      ],
      "metadata": {
        "id": "kxRTqzwLusWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_predictions[0, t, char_to_index[char]] = 1\n",
        "\n",
        "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
        "        next_index = sample(predictions,\n",
        "                                 temperature)\n",
        "        next_character = index_to_char[next_index]\n",
        "\n",
        "        generated += next_character\n",
        "        sentence = sentence[1:] + next_character\n",
        "    return generated"
      ],
      "metadata": {
        "id": "z33h42rrulxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(300,0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9eYXfuDvKcK",
        "outputId": "6adfce41-44de-4d2b-fc29-12cd73e0db08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xcuse that thou dost make in this delay\n",
            "the warthe he mard the soull and the have the will wear here the ward the have the will the will sheer his the have the thee the sour the hand the will for the thee sears,\n",
            "and the warther with the have the have the thee the will the ward the will the have the thee the will the will the will sheer th\n"
          ]
        }
      ]
    }
  ]
}